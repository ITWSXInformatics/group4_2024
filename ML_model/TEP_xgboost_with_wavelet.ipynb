{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cbef496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from xgboost import cv\n",
    "import pywt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d83ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''mean-std noralization'''\n",
    "def normalization_1(data):\n",
    "    \n",
    "    data_mean = np.mean(data,0)   #(72,)\n",
    "    data_std = np.std(data,0,ddof=1) \n",
    "    data_ = (data - data_mean)/data_std\n",
    "    \n",
    "    return data_, data_mean , data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad76de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''max-min noralization'''\n",
    "def normalization_2(data):\n",
    "    \n",
    "    data_min = np.min(data, 0)\n",
    "    data_max = np.max(data, 0) \n",
    "    data_ = (data - data_min) / (data_max - data_min + 1e-7)\n",
    "    \n",
    "    return data_, data_min , data_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375d7bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''one-hot label'''\n",
    "def to_categorical(y, num_classes=None):\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes))\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "055f0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''one-hot label'''\n",
    "# first arg: label\n",
    "# second arg: num of class \n",
    "def one_hot ( labels , Label_class ): \n",
    "    one_hot_label = np.array([[ int (i == int (labels[j])) for i in range (Label_class)] for j in range ( len (labels))])      \n",
    "    return one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab74bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''doing wavelet on input data, and spilt them by slidding window'''\n",
    "def add_window_wavelet(x, time_step, level_):\n",
    "    \n",
    "    x_window = []\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        series = x[i]\n",
    "        series_window = []\n",
    "        for j in range(int(series.shape[0]-time_step)):\n",
    "            dat = series[j: j+time_step]\n",
    "            fre_msg_var = []\n",
    "            for k in range(dat.shape[-1]): #var_num\n",
    "\n",
    "                coeffs = pywt.wavedec(dat[:,k], 'db1', level = level_, mode='sym')\n",
    "\n",
    "                fre_msg_ = []\n",
    "                for i in coeffs: # num_level * 3(mean + std + kurtosis)\n",
    "                    fre_msg_.append(np.mean(i, 0))\n",
    "                    fre_msg_.append(np.std(i, 0, ddof=1))\n",
    "#                     fre_msg_.append(stats.kurtosis(i))\n",
    "#                     print(f'fre_msg_: {np.mean(i, 0).shape}')\n",
    "#                     print(f'fre_msg_: {np.std(i, 0, ddof=1).shape}')\n",
    "\n",
    "                fre_msg_var.append(fre_msg_)\n",
    "#                 print(f'fre_msg_var: {np.array(fre_msg_var).shape}')\n",
    "\n",
    "            fre_msg_var = np.array(fre_msg_var)\n",
    "#             print(f'fre_msg_var: {fre_msg_var.shape}')\n",
    "            series_window.append(fre_msg_var.reshape([-1]))\n",
    "#             print(f'series_window: {np.array(series_window).shape}')\n",
    "        x_window.append(series_window)\n",
    "        print(f'x_window: {np.array(x_window).shape}')\n",
    "\n",
    "    return np.array(x_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bbfcdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''spilt labe by slidding window in order to be align with input data'''\n",
    "\n",
    "def label_add_window(x, time_step):\n",
    "    \n",
    "    x_window = []\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        series = x[i]\n",
    "        series_window = []\n",
    "        for j in range(int(series.shape[0]-time_step)):\n",
    "            dat = series[j: j+time_step]\n",
    "            series_window.append(dat[0])\n",
    "        x_window.append(series_window)\n",
    "    \n",
    "    return np.array(x_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6c2e53",
   "metadata": {},
   "source": [
    "# Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94b82093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of class, aka the amount of abnormality:  20\n",
      "['tep_train\\\\d01.dat', 'tep_train\\\\d02.dat', 'tep_train\\\\d04.dat', 'tep_train\\\\d06.dat', 'tep_train\\\\d07.dat', 'tep_train\\\\d08.dat', 'tep_train\\\\d10.dat', 'tep_train\\\\d11.dat', 'tep_train\\\\d12.dat', 'tep_train\\\\d13.dat', 'tep_train\\\\d14.dat', 'tep_train\\\\d17.dat', 'tep_train\\\\d18.dat', 'tep_train\\\\d19.dat', 'tep_train\\\\d20.dat', 'tep_train\\\\d21.dat', 'tep_train\\\\d16.dat', 'tep_train\\\\d03.dat', 'tep_train\\\\d15.dat', 'tep_train\\\\d05.dat']\n",
      "(20, 480, 52)\n",
      "(20, 480)\n"
     ]
    }
   ],
   "source": [
    "filedir = \"tep_train\"\n",
    "filenames = []\n",
    "train_feature = []\n",
    "train_lable = []\n",
    "\n",
    "for filename in os.listdir(filedir):\n",
    "    filenames.append(os.path.join(filedir,filename))\n",
    "\n",
    "# print(filenames)\n",
    "    \n",
    "filenames = [\n",
    "'tep_train\\\\d01.dat', 'tep_train\\\\d02.dat', \n",
    "'tep_train\\\\d04.dat', 'tep_train\\\\d06.dat', \n",
    "'tep_train\\\\d07.dat', 'tep_train\\\\d08.dat',\n",
    "'tep_train\\\\d10.dat', 'tep_train\\\\d11.dat', 'tep_train\\\\d12.dat', \n",
    "'tep_train\\\\d13.dat', 'tep_train\\\\d14.dat',\n",
    "'tep_train\\\\d17.dat', 'tep_train\\\\d18.dat', \n",
    "'tep_train\\\\d19.dat', 'tep_train\\\\d20.dat', 'tep_train\\\\d21.dat',\n",
    "'tep_train\\\\d16.dat', 'tep_train\\\\d03.dat', 'tep_train\\\\d15.dat', 'tep_train\\\\d05.dat',\n",
    "    \n",
    "# 'tep_train\\\\d09.dat'\n",
    "]\n",
    "print(\"num of class, aka the amount of abnormality: \",len(filenames))\n",
    "print(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    tep = np.genfromtxt(filenames[i])\n",
    "#     print(tep.shape)\n",
    "#     pca = PCA(n_components = 52)\n",
    "#     tep = pca.fit_transform(tep)     \n",
    "#     print(tep.shape)\n",
    "    train_feature.append(tep)\n",
    "    \n",
    "    label = np.ones(tep.shape[0])*i\n",
    "    train_lable.append(label)\n",
    "    \n",
    "train_feature = np.array(train_feature)\n",
    "train_lable = np.array(train_lable)\n",
    "\n",
    "print(train_feature.shape)\n",
    "print(train_lable.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab9a5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feature[0]\n",
    "\n",
    "# data_mean = np.mean(train_feature[0],0)   #(72,)\n",
    "# data_std = np.std(train_feature[0],0,ddof=1) \n",
    "\n",
    "# print(data_mean)\n",
    "# print(data_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef9cbb",
   "metadata": {},
   "source": [
    "# Test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa63eb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of class, aka the amount of abnormality:  20\n",
      "(20, 160, 52)\n",
      "(20, 500, 52)\n",
      "(20, 300, 52)\n",
      "(20, 160)\n",
      "(20, 500)\n",
      "(20, 300)\n"
     ]
    }
   ],
   "source": [
    "filedir = \"tep_test\"\n",
    "filenames_test = []\n",
    "\n",
    "test_normal = []\n",
    "test_fault = []\n",
    "val = []\n",
    "\n",
    "test_normal_label = []\n",
    "test_fault_label = []\n",
    "val_label = []\n",
    "\n",
    "for filename in os.listdir(filedir):\n",
    "    filenames_test.append(os.path.join(filedir,filename))\n",
    "    \n",
    "# print(filenames_test)\n",
    "\n",
    "filenames_test = [\n",
    "'tep_test\\\\d01_te.dat', 'tep_test\\\\d02_te.dat', \n",
    "'tep_test\\\\d04_te.dat', 'tep_test\\\\d06_te.dat', \n",
    "'tep_test\\\\d07_te.dat', 'tep_test\\\\d08_te.dat',\n",
    "'tep_test\\\\d10_te.dat', 'tep_test\\\\d11_te.dat', 'tep_test\\\\d12_te.dat', \n",
    "'tep_test\\\\d13_te.dat', 'tep_test\\\\d14_te.dat',  'tep_test\\\\d17_te.dat', 'tep_test\\\\d18_te.dat', \n",
    "'tep_test\\\\d19_te.dat', 'tep_test\\\\d20_te.dat', 'tep_test\\\\d21_te.dat',\n",
    "'tep_test\\\\d16_te.dat', 'tep_test\\\\d03_te.dat', 'tep_test\\\\d15_te.dat', 'tep_test\\\\d05_te.dat',\n",
    "\n",
    "# 'tep_test\\\\d09_te.dat'\n",
    "]\n",
    "    \n",
    "print(\"num of class, aka the amount of abnormality: \",len(filenames_test))\n",
    "\n",
    "for i in range(len(filenames_test)):\n",
    "    \n",
    "    tep = np.genfromtxt(filenames_test[i])\n",
    "#     pca = PCA(n_components = 52)\n",
    "#     tep = pca.fit_transform(tep)    \n",
    "    \n",
    "    test_normal_ = tep[:160]\n",
    "    test_normal.append(test_normal_)\n",
    "\n",
    "    val_ = tep[160:160+300]\n",
    "    val.append(val_)\n",
    "    \n",
    "    test_fault_ = tep[160+300:]\n",
    "    test_fault.append(test_fault_)\n",
    "\n",
    "    label = np.ones(tep.shape[0])*i\n",
    "    test_normal_label.append(label[:160])\n",
    "    test_fault_label.append(label[160+300:])\n",
    "    val_label.append(label[160:160+300])\n",
    "    \n",
    "test_normal = np.array(test_normal)\n",
    "test_fault = np.array(test_fault)\n",
    "val = np.array(val)\n",
    "test_normal_label = np.array(test_normal_label)\n",
    "test_fault_label = np.array(test_fault_label)\n",
    "val_label = np.array(val_label)\n",
    "\n",
    "print(test_normal.shape)\n",
    "print(test_fault.shape)\n",
    "print(val.shape)\n",
    "print(test_normal_label.shape)\n",
    "print(test_fault_label.shape)\n",
    "print(val_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b6c758d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 480)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d775898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 370)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step = 110\n",
    "train_y = label_add_window(train_lable, time_step)#.reshape([-1])\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f8db66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_window: (1, 370, 728)\n",
      "x_window: (2, 370, 728)\n",
      "x_window: (3, 370, 728)\n",
      "x_window: (4, 370, 728)\n",
      "x_window: (5, 370, 728)\n",
      "x_window: (6, 370, 728)\n",
      "x_window: (7, 370, 728)\n",
      "x_window: (8, 370, 728)\n",
      "x_window: (9, 370, 728)\n",
      "x_window: (10, 370, 728)\n",
      "x_window: (11, 370, 728)\n",
      "x_window: (12, 370, 728)\n",
      "x_window: (13, 370, 728)\n",
      "x_window: (14, 370, 728)\n",
      "x_window: (15, 370, 728)\n",
      "x_window: (16, 370, 728)\n",
      "x_window: (17, 370, 728)\n",
      "x_window: (18, 370, 728)\n",
      "x_window: (19, 370, 728)\n",
      "x_window: (20, 370, 728)\n",
      "(20, 370, 728)\n",
      "(7400, 728)\n",
      "train_x:  (7400, 728)\n",
      "train_y:  (7400,)\n"
     ]
    }
   ],
   "source": [
    "time_step = 110\n",
    "level = 6\n",
    "\n",
    "train_x = add_window_wavelet(train_feature, time_step, level)\n",
    "print(train_x.shape)\n",
    "train_x = train_x.reshape([-1, tep.shape[-1]*((level+1)*2) ])\n",
    "print(train_x.shape)\n",
    "\n",
    "# train_x, data_mean , data_std = normalization_1(train_x)\n",
    "train_y = label_add_window(train_lable, time_step).reshape([-1])\n",
    "\n",
    "# val_x = add_window_wavelet(val, time_step, level)\n",
    "# print(val_x.shape)\n",
    "# val_x = val_x.reshape([-1, tep.shape[-1]*((level+1)*2) ])\n",
    "# # val_x = (val_x - data_mean)/data_std\n",
    "# val_y = label_add_window(val_label, time_step).reshape([-1])\n",
    "\n",
    "# test_normal_x = add_window_wavelet(test_normal, time_step, level)\n",
    "# test_normal_x = test_normal_x.reshape([-1, tep.shape[-1]*((level+1)*2) ])\n",
    "# # test_normal_x = (test_normal_x - data_mean)/data_std\n",
    "# test_normal_y = label_add_window(test_normal_label, time_step).reshape([-1])\n",
    "\n",
    "# test_fault_x = add_window_wavelet(test_fault, time_step, level)\n",
    "# test_fault_x = test_fault_x.reshape([-1, tep.shape[-1]*((level+1)*2) ])\n",
    "# # test_fault_x = (test_fault_x - data_mean)/data_std\n",
    "# test_fault_y = label_add_window(test_fault_label, time_step).reshape([-1])\n",
    "\n",
    "\n",
    "print('train_x: ',train_x.shape)\n",
    "print('train_y: ',train_y.shape)\n",
    "# print('----------------------------------------------')\n",
    "# print('val_x: ',val_x.shape)\n",
    "# print('val_y: ',val_y.shape)\n",
    "# print('----------------------------------------------')\n",
    "# print('test_normal_x: ',test_normal_x.shape)\n",
    "# print('test_normal_y: ',test_normal_y.shape)\n",
    "# print('----------------------------------------------')\n",
    "# print('test_fault_x: ',test_fault_x.shape)\n",
    "# print('test_fault_y: ',test_fault_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23c5104f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "52*14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8677f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_x, train_y)\n",
    "dval = xgb.DMatrix(val_x, val_y)\n",
    "dtest_normal = xgb.DMatrix(test_normal_x, test_normal_y)\n",
    "dtest_fault = xgb.DMatrix(test_fault_x, test_fault_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ea12c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# params = {\n",
    "# 'objective': 'multi:softprob',\n",
    "# 'num_class': len(filenames),\n",
    "# 'seed': 0,\n",
    "# 'gamma': 0,\n",
    "# 'max_depth': 10, #10\n",
    "# # 'random_state': 0,\n",
    "# 'subsample': 0.075,\n",
    "# 'colsample_bytree': 0.8,\n",
    "# 'min_child_weight': 0, #3\n",
    "# 'lambda': 0.065, # 0.065\n",
    "# 'grow_policy': 'lossguide',\n",
    "# 'eta': 0.007,\n",
    "# 'eval_metric': ['merror'],\n",
    "# }\n",
    "\n",
    "# model = xgb.train(params, dtrain, \n",
    "#           num_boost_round = 4000, \n",
    "#           verbose_eval = 20, \n",
    "#           early_stopping_rounds = 200, \n",
    "#           evals=[(dtrain, 'train') , (dval, 'valid'), (dtest_normal, 'test_normal'), (dtest_fault, 'test_fault')],\n",
    "#           )\n",
    "\n",
    "# print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f9ff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "'objective': 'multi:softprob',\n",
    "'num_class': len(filenames),\n",
    "'seed': 0,\n",
    "'gamma': 0.5,\n",
    "'max_depth': 2, #10\n",
    "# 'random_state': 0,\n",
    "'subsample': 1,\n",
    "'min_child_weight': 3,\n",
    "'lambda': 3,\n",
    "'grow_policy': 'lossguide',\n",
    "'eta': 0.007,\n",
    "'eval_metric': ['merror'],\n",
    "}\n",
    "\n",
    "model = xgb.train(params, dtrain, \n",
    "          num_boost_round = 4000, \n",
    "          verbose_eval = 20, \n",
    "          early_stopping_rounds = 200, \n",
    "          evals=[(dtrain, 'train') , (dval, 'valid'), (dtest_normal, 'test_normal'), (dtest_fault, 'test_fault')],\n",
    "          )\n",
    "\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204b4e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(xgb.DMatrix(val_x))\n",
    "yprob = np.argmax(y_pred, axis=1)  # return the index of the biggest pro\n",
    "\n",
    "predictions = [round(value) for value in yprob]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(val_y, predictions)\n",
    "print(\"Accuracy: %.5f%%\" % (accuracy * 100.0))\n",
    "print('Recall: %.4f' % metrics.recall_score(val_y, predictions, average='macro'))\n",
    "print('F1-score: %.4f' % metrics.f1_score(val_y, predictions, average='macro'))\n",
    "print('Precesion: %.4f' % metrics.precision_score(val_y, predictions, average='macro'))\n",
    "print(\"confusion_matrix:\")\n",
    "print(confusion_matrix(val_y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5396aef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(xgb.DMatrix(test_normal_x))\n",
    "yprob = np.argmax(y_pred, axis=1)  # return the index of the biggest pro\n",
    "\n",
    "predictions = [round(value) for value in yprob]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(test_normal_y, predictions)\n",
    "print(\"Accuracy: %.5f%%\" % (accuracy * 100.0))\n",
    "print('Recall: %.4f' % metrics.recall_score(test_normal_y, predictions, average='macro'))\n",
    "print('F1-score: %.4f' % metrics.f1_score(test_normal_y, predictions, average='macro'))\n",
    "print('Precesion: %.4f' % metrics.precision_score(test_normal_y, predictions, average='macro'))\n",
    "print(\"confusion_matrix:\")\n",
    "print(confusion_matrix(test_normal_y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f3ac83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(xgb.DMatrix(test_fault_x))\n",
    "yprob = np.argmax(y_pred, axis=1)  # return the index of the biggest pro\n",
    "\n",
    "predictions = [round(value) for value in yprob]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(test_fault_y, predictions)\n",
    "print(\"Accuracy: %.5f%%\" % (accuracy * 100.0))\n",
    "print('Recall: %.4f' % metrics.recall_score(test_fault_y, predictions, average='macro'))\n",
    "print('F1-score: %.4f' % metrics.f1_score(test_fault_y, predictions, average='macro'))\n",
    "print('Precesion: %.4f' % metrics.precision_score(test_fault_y, predictions, average='macro'))\n",
    "print(\"confusion_matrix:\")\n",
    "print(confusion_matrix(test_fault_y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd2517b",
   "metadata": {},
   "source": [
    "# Feature importance with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e61a54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(model)\n",
    "plt.rcParams['figure.figsize'] = [14, 16]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d93e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccc6c3d9",
   "metadata": {},
   "source": [
    "# Fault root tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir = \"tep_train\"\n",
    "filenames = []\n",
    "train_feature = []\n",
    "train_lable = []\n",
    "\n",
    "normal_tep = np.genfromtxt('tep_train\\\\d01.dat')#.T[20:]\n",
    "train_feature.append(normal_tep)\n",
    "\n",
    "label = np.ones(normal_tep.shape[0])*0\n",
    "train_lable.append(label)\n",
    "\n",
    "for filename in os.listdir(filedir):\n",
    "    filenames.append(os.path.join(filedir,filename))\n",
    "\n",
    "# print(filenames)\n",
    "    \n",
    "filenames = ['tep_train\\\\d09.dat']\n",
    "print(\"num of class: \",len(filenames)+1)\n",
    "print(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    tep = np.genfromtxt(filenames[i])\n",
    "    train_feature.append(tep)\n",
    "    \n",
    "#     tep_mean = np.mean(tep,0)   #(72,)\n",
    "#     tep_std = np.std(tep,0,ddof=1) \n",
    "#     tep = (tep - tep_mean)/tep_std    \n",
    "    \n",
    "    label = np.ones(tep.shape[0])*1\n",
    "    train_lable.append(label)\n",
    "    \n",
    "train_feature = np.array(train_feature)\n",
    "train_lable = np.array(train_lable)\n",
    "\n",
    "print(train_feature.shape)\n",
    "print(train_lable.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f7fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtw_distance, _ = fastdtw(train_feature[0,:,0], train_feature[1,:,0],\n",
    "#                           dist=euclidean)\n",
    "\n",
    "dtw_set = []\n",
    "for i in range(train_feature.shape[2]):\n",
    "    dtw_distance, _ = fastdtw(train_feature[0,:,i], train_feature[1,:,i], dist=euclidean)\n",
    "    dtw_set.append(dtw_distance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82709d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('max ', np.max(dtw_set))\n",
    "# print('min ', np.min(dtw_set))\n",
    "# print('mean ', np.mean(dtw_set))\n",
    "# print('median ', np.median(dtw_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476b52b",
   "metadata": {},
   "source": [
    "1 vs 9\n",
    "\n",
    "max  23622.77699999998\n",
    "\n",
    "min  4.160972699999996\n",
    "\n",
    "mean  2597.777208840384\n",
    "\n",
    "median  262.61949999999996"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c81974",
   "metadata": {},
   "source": [
    "3 vs 9\n",
    "\n",
    "max  13202.499999999985\n",
    "\n",
    "min  3.434386200000003\n",
    "\n",
    "mean  818.9624737923073\n",
    "\n",
    "median  116.13500000000008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ae1bf",
   "metadata": {},
   "source": [
    "15 vs 9\n",
    "\n",
    "max  15233.199999999999\n",
    "\n",
    "min  3.2418722999999963\n",
    "\n",
    "mean  795.0586593711539\n",
    "\n",
    "median  103.68949999999998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2752c922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b7c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 vs 9\n",
    "\n",
    "plt.bar(range(len(dtw_set)), np.sort(dtw_set))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ce3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae61f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 vs 9\n",
    "\n",
    "plt.bar(range(len(dtw_set)), np.sort(dtw_set))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f31cce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89726810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 vs 9\n",
    "\n",
    "plt.bar(range(len(dtw_set)), np.sort(dtw_set))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aade7e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(train_feature.shape[2]):\n",
    "#     plt.rcParams['figure.figsize'] = [6, 4]\n",
    "#     plt.figure(i) \n",
    "#     plt.title(i)\n",
    "#     plt.plot(train_feature[0,:,i])\n",
    "#     plt.plot(train_feature[1,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27439b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(train_feature.shape[2]):\n",
    "#     plt.rcParams['figure.figsize'] = [6, 4]\n",
    "#     plt.figure(i) \n",
    "#     plt.title(i)\n",
    "#     plt.plot(train_feature[0,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e230c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(train_feature.shape[2]):\n",
    "#     plt.rcParams['figure.figsize'] = [6, 4]\n",
    "#     plt.figure(i) \n",
    "#     plt.title(i)\n",
    "#     plt.plot(train_feature[1,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a200a7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2adf340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.12",
   "language": "python",
   "name": "tf1.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
